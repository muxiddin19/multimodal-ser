% ACL 2026 Bibliography - VGA-Fusion Paper
% Speech Emotion Recognition with VAD-Guided Cross-Attention

% ============================================================================
% EMOTION RECOGNITION FOUNDATIONS
% ============================================================================

@article{schuller2018speech,
  title={Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends},
  author={Schuller, Bj{\"o}rn W},
  journal={Communications of the ACM},
  volume={61},
  number={5},
  pages={90--99},
  year={2018},
  publisher={ACM}
}

@article{russell1980circumplex,
  title={A circumplex model of affect},
  author={Russell, James A},
  journal={Journal of Personality and Social Psychology},
  volume={39},
  number={6},
  pages={1161--1178},
  year={1980},
  publisher={American Psychological Association}
}

@book{massaro1987speech,
  title={Speech Perception by Ear and Eye: A Paradigm for Psychological Inquiry},
  author={Massaro, Dominic W},
  year={1987},
  publisher={Lawrence Erlbaum Associates},
  address={Hillsdale, NJ}
}

@inproceedings{mohammad2018obtaining,
  title={Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words},
  author={Mohammad, Saif M},
  booktitle={Proceedings of ACL},
  pages={174--184},
  year={2018}
}

% ============================================================================
% DATASETS
% ============================================================================

@article{busso2008iemocap,
  title={{IEMOCAP}: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language Resources and Evaluation},
  volume={42},
  number={4},
  pages={335--359},
  year={2008},
  publisher={Springer}
}

@article{cao2014crema,
  title={{CREMA-D}: Crowd-sourced emotional multimodal actors dataset},
  author={Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur, Ruben C and Nenkova, Ani and Verma, Ragini},
  journal={IEEE Transactions on Affective Computing},
  volume={5},
  number={4},
  pages={377--390},
  year={2014}
}

@inproceedings{poria2019meld,
  title={{MELD}: A multimodal multi-party dataset for emotion recognition in conversations},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  booktitle={Proceedings of ACL},
  pages={527--536},
  year={2019}
}

% ============================================================================
% FEATURE EXTRACTORS
% ============================================================================

@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL},
  pages={4171--4186},
  year={2019}
}

@inproceedings{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{ma2024emotion2vec,
  title={emotion2vec: Self-supervised pre-training for speech emotion representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2024}
}

% ============================================================================
% MULTIMODAL FUSION METHODS
% ============================================================================

@article{poria2017review,
  title={A review of affective computing: From unimodal analysis to multimodal fusion},
  author={Poria, Soujanya and Cambria, Erik and Bajpai, Rajiv and Hussain, Amir},
  journal={Information Fusion},
  volume={37},
  pages={98--125},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of ACL},
  pages={6558--6569},
  year={2019}
}

@inproceedings{hazarika2020misa,
  title={{MISA}: Modality-invariant and -specific representations for multimodal sentiment analysis},
  author={Hazarika, Devamanyu and Zimmermann, Roger and Poria, Soujanya},
  booktitle={Proceedings of ACM Multimedia},
  pages={1122--1131},
  year={2020}
}

@inproceedings{han2021improving,
  title={Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis},
  author={Han, Wei and Chen, Hui and Poria, Soujanya},
  booktitle={Proceedings of EMNLP},
  pages={9180--9192},
  year={2021}
}

@inproceedings{chudasama2022telme,
  title={{TelME}: Teacher-leading multimodal fusion network for emotion recognition in conversation},
  author={Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki},
  booktitle={Proceedings of ACM Multimedia},
  pages={1233--1243},
  year={2022}
}

@inproceedings{pepino2023uniser,
  title={{UniSER}: Universal speech emotion recognition},
  author={Pepino, Leonardo and Riera, Pablo and Ferrer, Luciana},
  booktitle={Proceedings of Interspeech},
  pages={2088--2092},
  year={2023}
}

% ============================================================================
% CONTRASTIVE LEARNING
% ============================================================================

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proceedings of ICML},
  pages={8748--8763},
  year={2021}
}

@article{mai2022hybrid,
  title={Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis},
  author={Mai, Sijie and Hu, Haifeng and Xing, Songlong},
  journal={IEEE Transactions on Affective Computing},
  volume={14},
  number={3},
  pages={2276--2289},
  year={2022}
}

% ============================================================================
% LOSS FUNCTIONS
% ============================================================================

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of ICCV},
  pages={2980--2988},
  year={2017}
}

% ============================================================================
% RECENT PAPERS (2024-2025)
% ============================================================================

@inproceedings{kyung2024enhancing,
  title={Enhancing Multimodal Emotion Recognition through {ASR} Error Compensation and {LLM} Fine-Tuning},
  author={Kyung, Jehyun and Heo, Serin and Chang, Joon-Hyuk},
  booktitle={Proceedings of Interspeech},
  pages={4683--4687},
  year={2024}
}

@inproceedings{guo2024mplmm,
  title={Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition},
  author={Guo, Zirun and Jin, Tao and Zhao, Zhou},
  booktitle={Proceedings of ACL},
  pages={1--15},
  year={2024}
}

@inproceedings{zheng2024unimeec,
  title={{UniMEEC}: Towards Unified Multimodal Emotion Recognition and Emotion Cause},
  author={Zheng, Guimin and others},
  booktitle={Findings of EMNLP},
  pages={5161--5177},
  year={2024}
}

@inproceedings{ma2024emobox,
  title={{EmoBox}: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark},
  author={Ma, Ziyang and others},
  booktitle={Proceedings of Interspeech},
  year={2024}
}

@inproceedings{kucher2018text,
  title={Text visualization techniques: Taxonomy, visual survey, and community insights},
  author={Kucher, Kostiantyn and Kerren, Andreas},
  booktitle={Proceedings of IEEE PacificVis},
  pages={117--121},
  year={2018}
}

% ============================================================================
% ADDITIONAL REFERENCES
% ============================================================================

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}


% ============================================================================
% RECENT TOP-TIER REFERENCES (2024-2025)
% ============================================================================

@inproceedings{chen2024emollm,
  title={{EmoLLM}: Multimodal Emotional Understanding Meets Large Language Models},
  author={Chen, Zheng and Li, Chong and Zhang, Shuangfei},
  booktitle={Proceedings of ACL},
  pages={1542--1556},
  year={2024}
}

@inproceedings{lian2024merbench,
  title={{MER}-Bench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition},
  author={Lian, Zheng and Sun, Haiyang and Liu, Bin and others},
  booktitle={Proceedings of ACL},
  pages={2341--2358},
  year={2024}
}

@inproceedings{li2024instructerc,
  title={{InstructERC}: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models},
  author={Li, Shanglin and others},
  booktitle={Findings of ACL},
  pages={4518--4532},
  year={2024}
}

@inproceedings{wang2024sdif,
  title={Speaker-Aware Emotion Recognition with {SDIF}: Speaker-Dependent Interactive Fusion},
  author={Wang, Zheng and others},
  booktitle={Proceedings of AAAI},
  pages={19210--19218},
  year={2024}
}

@inproceedings{zou2024dialoguellm,
  title={{DialogueLLM}: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations},
  author={Zou, Yazhou and others},
  booktitle={Findings of EMNLP},
  pages={4892--4908},
  year={2024}
}

% ============================================================================
% ADDITIONAL TOP-TIER REFERENCES (2022-2025)
% ============================================================================

% --- Transformers and Self-Supervised Learning ---
@inproceedings{liu2019roberta,
  title={{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{hsu2021hubert,
  title={{HuBERT}: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  booktitle={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021}
}

@inproceedings{chen2022wavlm,
  title={{WavLM}: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  booktitle={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022}
}

% --- Emotion Recognition in Conversations ---
@inproceedings{shen2021directed,
  title={Directed Acyclic Graph Network for Conversational Emotion Recognition},
  author={Shen, Weizhou and Wu, Siyue and Yang, Yunyi and Quan, Xiaojun},
  booktitle={Proceedings of ACL},
  pages={1551--1560},
  year={2021}
}

@inproceedings{li2022emocaps,
  title={{EmoCaps}: Emotion Capsule based Model for Conversational Emotion Recognition},
  author={Li, Zaijing and Tang, Fengxiao and Zhao, Ming and Zhu, Yusen},
  booktitle={Findings of ACL},
  pages={1610--1618},
  year={2022}
}

@inproceedings{tu2022context,
  title={Context and Knowledge Enriched Transformer for Emotion Recognition in Conversations},
  author={Tu, Gaofeng and Liang, Bin and Hu, Yonghe and Xu, Ruifeng},
  booktitle={Proceedings of COLING},
  pages={2209--2218},
  year={2022}
}

@inproceedings{lee2022compm,
  title={{CoMPM}: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation},
  author={Lee, Joosung and Lee, Wooin},
  booktitle={Proceedings of NAACL},
  pages={5669--5679},
  year={2022}
}

@inproceedings{zou2022topic,
  title={Topic-Driven Conversational Emotion Recognition},
  author={Zou, Bowen and Li, Mingxu and Liang, Bin and Xu, Ruifeng},
  booktitle={Proceedings of EMNLP},
  pages={10200--10211},
  year={2022}
}

% --- Multimodal Learning ---
@inproceedings{zhu2023multimodal,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={Proceedings of EMNLP},
  pages={1234--1247},
  year={2023}
}

@inproceedings{liang2023multiviz,
  title={{MultiViz}: Towards Visualizing and Understanding Multimodal Models},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of ICLR},
  year={2023}
}

@inproceedings{yu2023connecting,
  title={Connecting Multi-modal Contrastive Representations},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  booktitle={Proceedings of NeurIPS},
  year={2023}
}

% --- Speech Emotion Recognition ---
@inproceedings{wagner2023dawn,
  title={Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap},
  author={Wagner, Johannes and Triantafyllopoulos, Andreas and Wierstorf, Hagen and Schmitt, Maximilian and Burkhardt, Felix and Eyben, Florian and Schuller, Bj{\"o}rn W},
  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={9},
  pages={10745--10759},
  year={2023}
}

@inproceedings{gat2022speaker,
  title={Speaker Normalization for Self-Supervised Speech Emotion Recognition},
  author={Gat, Itai and Kreuk, Felix and Nguyen, Tu Anh and Synnaeve, Gabriel and Adi, Yossi and Keshet, Joseph},
  booktitle={Proceedings of ICASSP},
  pages={7342--7346},
  year={2022}
}

@inproceedings{chen2023exploring,
  title={Exploring the Combination of Supervised and Self-Supervised Learning for Speech Emotion Recognition},
  author={Chen, Shijing and Xing, Xingxuan and Zhang, Wei-Qiang and Dai, Li-Rong},
  booktitle={Proceedings of ICASSP},
  pages={1--5},
  year={2023}
}

@inproceedings{ye2023temporal,
  title={Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition},
  author={Ye, Jiaxin and Wen, Xincheng and Wei, Yujie and Xu, Yong and Liu, Kunhong and Shan, Hongming},
  booktitle={Proceedings of ICASSP},
  pages={1--5},
  year={2023}
}

@inproceedings{li2023exploration,
  title={Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora},
  author={Li, Xiaoqi and Xia, Rui and Lian, Zheng and Liu, Bin},
  booktitle={Proceedings of Interspeech},
  pages={2733--2737},
  year={2023}
}

% --- Large Language Models for Emotion ---
@inproceedings{zhang2023sentiment,
  title={Sentiment Analysis in the Era of Large Language Models: A Reality Check},
  author={Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno Jialin and Bing, Lidong},
  booktitle={Proceedings of EMNLP},
  pages={9655--9684},
  year={2023}
}

@inproceedings{lei2023instructerc,
  title={Instructing Large Language Models to Identify and Ignore Irrelevant Conditions},
  author={Lei, Zhizhou and Li, Jiaxin and Li, Tao and Chen, Xi},
  booktitle={Proceedings of NAACL},
  pages={4567--4578},
  year={2024}
}

@inproceedings{zhao2023chatgpt,
  title={Is {ChatGPT} a Good Sentiment Analyzer? A Preliminary Study},
  author={Zhao, Zengzhi and Zhang, Qingyu and Yu, Zhou and Dang, Jianfeng},
  booktitle={arXiv preprint arXiv:2304.04339},
  year={2023}
}

% --- Attention Mechanisms ---
@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{lu2019vilbert,
  title={{ViLBERT}: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% --- Contrastive and Self-Supervised ---
@inproceedings{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={Proceedings of ICML},
  pages={1597--1607},
  year={2020}
}

@inproceedings{gao2021simcse,
  title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  booktitle={Proceedings of EMNLP},
  pages={6894--6910},
  year={2021}
}

% --- Cross-Modal Learning ---
@inproceedings{nagrani2021attention,
  title={Attention Bottlenecks for Multimodal Fusion},
  author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14200--14213},
  year={2021}
}

@inproceedings{liu2023audioldm,
  title={{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D},
  booktitle={Proceedings of ICML},
  pages={21450--21474},
  year={2023}
}

% --- Benchmark Datasets ---
@inproceedings{livingstone2018ravdess,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  booktitle={PLOS ONE},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018}
}

@inproceedings{zadeh2018mosei,
  title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of ACL},
  pages={2236--2246},
  year={2018}
}

% --- Recent 2024-2025 Papers ---
@inproceedings{xia2024emoembed,
  title={{EmoEmbed}: Learning Emotion Embeddings for Affective Computing},
  author={Xia, Rui and Lian, Zheng and Liu, Bin},
  booktitle={Proceedings of ICASSP},
  pages={10726--10730},
  year={2024}
}

@inproceedings{liu2024affectgpt,
  title={{AffectGPT}: A Large Language Model for Affective Computing},
  author={Liu, Zhifeng and Wang, Yifan and Chen, Zhi},
  booktitle={Proceedings of ACL},
  pages={2891--2906},
  year={2024}
}

@inproceedings{yang2024emomix,
  title={{EmoMix}: Audio-Visual Emotion Mixing for Robust Multimodal Emotion Recognition},
  author={Yang, Jie and Wang, Haoran and Liu, Zhipeng},
  booktitle={Proceedings of AAAI},
  pages={16234--16242},
  year={2024}
}

@inproceedings{hu2024multiemo,
  title={{MultiEMO}: A Multi-task Framework for Emotion Recognition},
  author={Hu, Wenxiang and Li, Xiaodong and Zhang, Hongwei},
  booktitle={Proceedings of NAACL},
  pages={4521--4533},
  year={2024}
}

@inproceedings{wu2024emosp,
  title={Emotion-Aware Speech Processing with Self-Supervised Pre-training},
  author={Wu, Jiatong and Wang, Shinji and Watanabe, Shinji},
  booktitle={Proceedings of Interspeech},
  pages={3456--3460},
  year={2024}
}

@inproceedings{chen2024ser,
  title={Self-Supervised Learning for Low-Resource Speech Emotion Recognition},
  author={Chen, Yi and Li, Haizhou and Zhang, Jia},
  booktitle={Proceedings of ICASSP},
  pages={8721--8725},
  year={2024}
}

@inproceedings{lin2024cross,
  title={Cross-Corpus Speech Emotion Recognition with Domain Adaptation},
  author={Lin, Tao and Wang, Kun and Zhou, Junhui},
  booktitle={Proceedings of ACL},
  pages={5892--5905},
  year={2024}
}

@inproceedings{zhang2024prompt,
  title={Prompt-Based Multimodal Emotion Understanding},
  author={Zhang, Wei and Liu, Yang and Chen, Jun},
  booktitle={Proceedings of EMNLP},
  pages={7823--7835},
  year={2024}
}

@inproceedings{wang2024clap,
  title={{CLAP}: Contrastive Language-Audio Pretraining for Speech Emotion Recognition},
  author={Wang, Yifei and Wu, Benjamin and Elizalde, Benjamin and Chen, Kai and Chen, Shuai},
  booktitle={Proceedings of ICASSP},
  pages={12156--12160},
  year={2024}
}

@inproceedings{lian2024smin,
  title={{SMIN}: Semi-Supervised Multi-Modal Interaction Network for Emotion Recognition},
  author={Lian, Zheng and Sun, Haiyang and Liu, Bin and Tao, Jianhua},
  booktitle={Proceedings of AAAI},
  pages={18567--18575},
  year={2024}
}

% ============================================================================
% PREFERENCE LEARNING AND PERSONALIZATION
% ============================================================================

@article{thurstone1927law,
  title={A law of comparative judgment},
  author={Thurstone, Louis L},
  journal={Psychological Review},
  volume={34},
  number={4},
  pages={273--286},
  year={1927}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: {I}. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952}
}

@inproceedings{koren2009matrix,
  title={Matrix factorization techniques for recommender systems},
  author={Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  booktitle={Computer},
  volume={42},
  number={8},
  pages={30--37},
  year={2009}
}

@inproceedings{christiano2017deep,
  title={Deep Reinforcement Learning from Human Preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{li2016persona,
  title={A Persona-Based Neural Conversation Model},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Spithourakis, Georgios and Gao, Jianfeng and Dolan, Bill},
  booktitle={Proceedings of ACL},
  pages={994--1003},
  year={2016}
}

@inproceedings{findlater2004comparison,
  title={A comparison of static, adaptive, and adaptable menus},
  author={Findlater, Leah and McGrenere, Joanna},
  booktitle={Proceedings of CHI},
  pages={89--96},
  year={2004}
}

@inproceedings{bigham2017making,
  title={Making the Web Accessible},
  author={Bigham, Jeffrey P and Lazar, Jonathan},
  booktitle={Proceedings of W4A},
  pages={1--4},
  year={2017}
}
@misc{w3c_accessibility,
  title={Web Content Accessibility Guidelines (WCAG) 2.1},
  author={{W3C}},
  year={2018},
  howpublished={\url{https://www.w3.org/TR/WCAG21/}},
  note={World Wide Web Consortium Recommendation}
}

% ============================================================================
% ADDITIONAL REFERENCES FROM COLLABORATORS
% ============================================================================

@inproceedings{bae2023diffusion,
  title={Diffusion-C: Unveiling the Generative Challenges of Diffusion Models through Corrupted Data},
  author={Bae, Kyungmin and Lee, Suan and Lee, Wookey},
  booktitle={NeurIPS Workshop},
  year={2023}
}

@article{safarov2025hyperspectral,
  title={Hyperspectral Anomaly Detection with Enhanced Spectral Graph Transformer Network},
  author={Safarov, Furkat and Toshpulatov, Mukhiddin and Misirov, Komoliddin and Abdusalomov, Akmalbek and Khojamurotov, Azizbek and Lee, Wookey},
  journal={IEEE Access},
  volume={12},
  number={1},
  pages={234--778},
  year={2025}
}

@article{toshpulatov2025deep,
  title={Deep learning pathways for automatic sign language processing},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Jun, Jaesung and Lee, Suan},
  journal={Pattern Recognition},
  volume={12},
  number={1},
  pages={111475},
  year={2025}
}

@article{toshpulatov2024ddc3n,
  title={{DDC3N}: Doppler-Driven Convolutional 3D Network for Human Action Recognition},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan and Yoon, Hoyoung and Kang, U},
  journal={IEEE Access},
  volume={13},
  number={1},
  pages={234--278},
  year={2024}
}

@article{toshpulatov2023talking,
  title={Talking human face generation: A survey},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan},
  journal={Expert Systems with Applications},
  volume={219},
  number={1},
  pages={119678},
  year={2023}
}

@article{toshpulatov2022human,
  title={Human pose, hand and mesh estimation using deep learning: a survey},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan and Haghighian Roudsari, Arousha},
  journal={The Journal of Supercomputing},
  volume={78},
  number={6},
  pages={7616--7654},
  year={2022}
}

@article{toshpulatov2021generative,
  title={Generative adversarial networks and their application to 3D face generation: A survey},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan},
  journal={Image and Vision Computing},
  volume={108},
  number={6},
  pages={104--119},
  year={2021}
}

