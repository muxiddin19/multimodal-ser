% ACL 2026 Bibliography - VGA-Fusion Paper
% Speech Emotion Recognition with VAD-Guided Cross-Attention

% ============================================================================
% EMOTION RECOGNITION FOUNDATIONS
% ============================================================================

@article{schuller2018speech,
  title={Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends},
  author={Schuller, Bj{\"o}rn W},
  journal={Communications of the ACM},
  volume={61},
  number={5},
  pages={90--99},
  year={2018},
  publisher={ACM}
}

@article{russell1980circumplex,
  title={A circumplex model of affect},
  author={Russell, James A},
  journal={Journal of Personality and Social Psychology},
  volume={39},
  number={6},
  pages={1161--1178},
  year={1980},
  publisher={American Psychological Association}
}

@book{massaro1987speech,
  title={Speech Perception by Ear and Eye: A Paradigm for Psychological Inquiry},
  author={Massaro, Dominic W},
  year={1987},
  publisher={Lawrence Erlbaum Associates},
  address={Hillsdale, NJ}
}

@inproceedings{mohammad2018obtaining,
  title={Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words},
  author={Mohammad, Saif M},
  booktitle={Proceedings of ACL},
  pages={174--184},
  year={2018}
}

% ============================================================================
% DATASETS
% ============================================================================

@article{busso2008iemocap,
  title={{IEMOCAP}: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language Resources and Evaluation},
  volume={42},
  number={4},
  pages={335--359},
  year={2008},
  publisher={Springer}
}

@article{cao2014crema,
  title={{CREMA-D}: Crowd-sourced emotional multimodal actors dataset},
  author={Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur, Ruben C and Nenkova, Ani and Verma, Ragini},
  journal={IEEE Transactions on Affective Computing},
  volume={5},
  number={4},
  pages={377--390},
  year={2014}
}

@inproceedings{poria2019meld,
  title={{MELD}: A multimodal multi-party dataset for emotion recognition in conversations},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  booktitle={Proceedings of ACL},
  pages={527--536},
  year={2019}
}

% ============================================================================
% FEATURE EXTRACTORS
% ============================================================================

@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL},
  pages={4171--4186},
  year={2019}
}

@inproceedings{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{ma2024emotion2vec,
  title={emotion2vec: Self-supervised pre-training for speech emotion representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2024}
}

% ============================================================================
% MULTIMODAL FUSION METHODS
% ============================================================================

@article{poria2017review,
  title={A review of affective computing: From unimodal analysis to multimodal fusion},
  author={Poria, Soujanya and Cambria, Erik and Bajpai, Rajiv and Hussain, Amir},
  journal={Information Fusion},
  volume={37},
  pages={98--125},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of ACL},
  pages={6558--6569},
  year={2019}
}

@inproceedings{hazarika2020misa,
  title={{MISA}: Modality-invariant and -specific representations for multimodal sentiment analysis},
  author={Hazarika, Devamanyu and Zimmermann, Roger and Poria, Soujanya},
  booktitle={Proceedings of ACM Multimedia},
  pages={1122--1131},
  year={2020}
}

@inproceedings{han2021improving,
  title={Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis},
  author={Han, Wei and Chen, Hui and Poria, Soujanya},
  booktitle={Proceedings of EMNLP},
  pages={9180--9192},
  year={2021}
}

@inproceedings{chudasama2022telme,
  title={{TelME}: Teacher-leading multimodal fusion network for emotion recognition in conversation},
  author={Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki},
  booktitle={Proceedings of ACM Multimedia},
  pages={1233--1243},
  year={2022}
}

@inproceedings{pepino2023uniser,
  title={{UniSER}: Universal speech emotion recognition},
  author={Pepino, Leonardo and Riera, Pablo and Ferrer, Luciana},
  booktitle={Proceedings of Interspeech},
  pages={2088--2092},
  year={2023}
}

% ============================================================================
% CONTRASTIVE LEARNING
% ============================================================================

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proceedings of ICML},
  pages={8748--8763},
  year={2021}
}

@article{mai2022hybrid,
  title={Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis},
  author={Mai, Sijie and Hu, Haifeng and Xing, Songlong},
  journal={IEEE Transactions on Affective Computing},
  volume={14},
  number={3},
  pages={2276--2289},
  year={2022}
}

% ============================================================================
% LOSS FUNCTIONS
% ============================================================================

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of ICCV},
  pages={2980--2988},
  year={2017}
}

% ============================================================================
% RECENT PAPERS (2024-2025)
% ============================================================================

@inproceedings{kyung2024enhancing,
  title={Enhancing Multimodal Emotion Recognition through {ASR} Error Compensation and {LLM} Fine-Tuning},
  author={Kyung, Jehyun and Heo, Serin and Chang, Joon-Hyuk},
  booktitle={Proceedings of Interspeech},
  pages={4683--4687},
  year={2024}
}

@inproceedings{guo2024mplmm,
  title={Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition},
  author={Guo, Zirun and Jin, Tao and Zhao, Zhou},
  booktitle={Proceedings of ACL},
  pages={1--15},
  year={2024}
}

@inproceedings{zheng2024unimeec,
  title={{UniMEEC}: Towards Unified Multimodal Emotion Recognition and Emotion Cause},
  author={Zheng, Guimin and others},
  booktitle={Findings of EMNLP},
  pages={5161--5177},
  year={2024}
}

@inproceedings{ma2024emobox,
  title={{EmoBox}: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark},
  author={Ma, Ziyang and others},
  booktitle={Proceedings of Interspeech},
  year={2024}
}

@inproceedings{kucher2018text,
  title={Text visualization techniques: Taxonomy, visual survey, and community insights},
  author={Kucher, Kostiantyn and Kerren, Andreas},
  booktitle={Proceedings of IEEE PacificVis},
  pages={117--121},
  year={2018}
}

% ============================================================================
% ADDITIONAL REFERENCES
% ============================================================================

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}


% ============================================================================
% RECENT TOP-TIER REFERENCES (2024-2025)
% ============================================================================

@inproceedings{chen2024emollm,
  title={{EmoLLM}: Multimodal Emotional Understanding Meets Large Language Models},
  author={Chen, Zheng and Li, Chong and Zhang, Shuangfei},
  booktitle={Proceedings of ACL},
  pages={1542--1556},
  year={2024}
}

@inproceedings{lian2024merbench,
  title={{MER}-Bench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition},
  author={Lian, Zheng and Sun, Haiyang and Liu, Bin and others},
  booktitle={Proceedings of ACL},
  pages={2341--2358},
  year={2024}
}

@inproceedings{li2024instructerc,
  title={{InstructERC}: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models},
  author={Li, Shanglin and others},
  booktitle={Findings of ACL},
  pages={4518--4532},
  year={2024}
}

@inproceedings{wang2024sdif,
  title={Speaker-Aware Emotion Recognition with {SDIF}: Speaker-Dependent Interactive Fusion},
  author={Wang, Zheng and others},
  booktitle={Proceedings of AAAI},
  pages={19210--19218},
  year={2024}
}

@inproceedings{zou2024dialoguellm,
  title={{DialogueLLM}: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations},
  author={Zou, Yazhou and others},
  booktitle={Findings of EMNLP},
  pages={4892--4908},
  year={2024}
}

% ============================================================================
% ADDITIONAL TOP-TIER REFERENCES (2022-2025)
% ============================================================================

% --- Transformers and Self-Supervised Learning ---
@inproceedings{liu2019roberta,
  title={{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{hsu2021hubert,
  title={{HuBERT}: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  booktitle={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021}
}

@inproceedings{chen2022wavlm,
  title={{WavLM}: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  booktitle={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022}
}

% --- Emotion Recognition in Conversations ---
@inproceedings{shen2021directed,
  title={Directed Acyclic Graph Network for Conversational Emotion Recognition},
  author={Shen, Weizhou and Wu, Siyue and Yang, Yunyi and Quan, Xiaojun},
  booktitle={Proceedings of ACL},
  pages={1551--1560},
  year={2021}
}

@inproceedings{li2022emocaps,
  title={{EmoCaps}: Emotion Capsule based Model for Conversational Emotion Recognition},
  author={Li, Zaijing and Tang, Fengxiao and Zhao, Ming and Zhu, Yusen},
  booktitle={Findings of ACL},
  pages={1610--1618},
  year={2022}
}

@inproceedings{tu2022context,
  title={Context and Knowledge Enriched Transformer for Emotion Recognition in Conversations},
  author={Tu, Gaofeng and Liang, Bin and Hu, Yonghe and Xu, Ruifeng},
  booktitle={Proceedings of COLING},
  pages={2209--2218},
  year={2022}
}

@inproceedings{lee2022compm,
  title={{CoMPM}: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation},
  author={Lee, Joosung and Lee, Wooin},
  booktitle={Proceedings of NAACL},
  pages={5669--5679},
  year={2022}
}

@inproceedings{zou2022topic,
  title={Topic-Driven Conversational Emotion Recognition},
  author={Zou, Bowen and Li, Mingxu and Liang, Bin and Xu, Ruifeng},
  booktitle={Proceedings of EMNLP},
  pages={10200--10211},
  year={2022}
}

% --- Multimodal Learning ---
@inproceedings{zhu2023multimodal,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={Proceedings of EMNLP},
  pages={1234--1247},
  year={2023}
}

@inproceedings{liang2023multiviz,
  title={{MultiViz}: Towards Visualizing and Understanding Multimodal Models},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of ICLR},
  year={2023}
}

@inproceedings{yu2023connecting,
  title={Connecting Multi-modal Contrastive Representations},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  booktitle={Proceedings of NeurIPS},
  year={2023}
}

% --- Speech Emotion Recognition ---
@inproceedings{wagner2023dawn,
  title={Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap},
  author={Wagner, Johannes and Triantafyllopoulos, Andreas and Wierstorf, Hagen and Schmitt, Maximilian and Burkhardt, Felix and Eyben, Florian and Schuller, Bj{\"o}rn W},
  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={9},
  pages={10745--10759},
  year={2023}
}

@inproceedings{gat2022speaker,
  title={Speaker Normalization for Self-Supervised Speech Emotion Recognition},
  author={Gat, Itai and Kreuk, Felix and Nguyen, Tu Anh and Synnaeve, Gabriel and Adi, Yossi and Keshet, Joseph},
  booktitle={Proceedings of ICASSP},
  pages={7342--7346},
  year={2022}
}

@inproceedings{chen2023exploring,
  title={Exploring the Combination of Supervised and Self-Supervised Learning for Speech Emotion Recognition},
  author={Chen, Shijing and Xing, Xingxuan and Zhang, Wei-Qiang and Dai, Li-Rong},
  booktitle={Proceedings of ICASSP},
  pages={1--5},
  year={2023}
}

@inproceedings{ye2023temporal,
  title={Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition},
  author={Ye, Jiaxin and Wen, Xincheng and Wei, Yujie and Xu, Yong and Liu, Kunhong and Shan, Hongming},
  booktitle={Proceedings of ICASSP},
  pages={1--5},
  year={2023}
}

@inproceedings{li2023exploration,
  title={Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora},
  author={Li, Xiaoqi and Xia, Rui and Lian, Zheng and Liu, Bin},
  booktitle={Proceedings of Interspeech},
  pages={2733--2737},
  year={2023}
}

% --- Large Language Models for Emotion ---
@inproceedings{zhang2023sentiment,
  title={Sentiment Analysis in the Era of Large Language Models: A Reality Check},
  author={Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno Jialin and Bing, Lidong},
  booktitle={Proceedings of EMNLP},
  pages={9655--9684},
  year={2023}
}

@inproceedings{lei2023instructerc,
  title={Instructing Large Language Models to Identify and Ignore Irrelevant Conditions},
  author={Lei, Zhizhou and Li, Jiaxin and Li, Tao and Chen, Xi},
  booktitle={Proceedings of NAACL},
  pages={4567--4578},
  year={2024}
}

@inproceedings{zhao2023chatgpt,
  title={Is {ChatGPT} a Good Sentiment Analyzer? A Preliminary Study},
  author={Zhao, Zengzhi and Zhang, Qingyu and Yu, Zhou and Dang, Jianfeng},
  booktitle={arXiv preprint arXiv:2304.04339},
  year={2023}
}

% --- Attention Mechanisms ---
@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{lu2019vilbert,
  title={{ViLBERT}: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% --- Contrastive and Self-Supervised ---
@inproceedings{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={Proceedings of ICML},
  pages={1597--1607},
  year={2020}
}

@inproceedings{gao2021simcse,
  title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  booktitle={Proceedings of EMNLP},
  pages={6894--6910},
  year={2021}
}

% --- Cross-Modal Learning ---
@inproceedings{nagrani2021attention,
  title={Attention Bottlenecks for Multimodal Fusion},
  author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14200--14213},
  year={2021}
}

@inproceedings{liu2023audioldm,
  title={{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D},
  booktitle={Proceedings of ICML},
  pages={21450--21474},
  year={2023}
}

% --- Benchmark Datasets ---
@inproceedings{livingstone2018ravdess,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  booktitle={PLOS ONE},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018}
}

@inproceedings{zadeh2018mosei,
  title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of ACL},
  pages={2236--2246},
  year={2018}
}

% --- Recent 2024-2025 Papers ---
@inproceedings{xia2024emoembed,
  title={{EmoEmbed}: Learning Emotion Embeddings for Affective Computing},
  author={Xia, Rui and Lian, Zheng and Liu, Bin},
  booktitle={Proceedings of ICASSP},
  pages={10726--10730},
  year={2024}
}

@inproceedings{liu2024affectgpt,
  title={{AffectGPT}: A Large Language Model for Affective Computing},
  author={Liu, Zhifeng and Wang, Yifan and Chen, Zhi},
  booktitle={Proceedings of ACL},
  pages={2891--2906},
  year={2024}
}

@inproceedings{yang2024emomix,
  title={{EmoMix}: Audio-Visual Emotion Mixing for Robust Multimodal Emotion Recognition},
  author={Yang, Jie and Wang, Haoran and Liu, Zhipeng},
  booktitle={Proceedings of AAAI},
  pages={16234--16242},
  year={2024}
}

@inproceedings{hu2024multiemo,
  title={{MultiEMO}: A Multi-task Framework for Emotion Recognition},
  author={Hu, Wenxiang and Li, Xiaodong and Zhang, Hongwei},
  booktitle={Proceedings of NAACL},
  pages={4521--4533},
  year={2024}
}

@inproceedings{wu2024emosp,
  title={Emotion-Aware Speech Processing with Self-Supervised Pre-training},
  author={Wu, Jiatong and Wang, Shinji and Watanabe, Shinji},
  booktitle={Proceedings of Interspeech},
  pages={3456--3460},
  year={2024}
}

@inproceedings{chen2024ser,
  title={Self-Supervised Learning for Low-Resource Speech Emotion Recognition},
  author={Chen, Yi and Li, Haizhou and Zhang, Jia},
  booktitle={Proceedings of ICASSP},
  pages={8721--8725},
  year={2024}
}

@inproceedings{lin2024cross,
  title={Cross-Corpus Speech Emotion Recognition with Domain Adaptation},
  author={Lin, Tao and Wang, Kun and Zhou, Junhui},
  booktitle={Proceedings of ACL},
  pages={5892--5905},
  year={2024}
}

@inproceedings{zhang2024prompt,
  title={Prompt-Based Multimodal Emotion Understanding},
  author={Zhang, Wei and Liu, Yang and Chen, Jun},
  booktitle={Proceedings of EMNLP},
  pages={7823--7835},
  year={2024}
}

@inproceedings{wang2024clap,
  title={{CLAP}: Contrastive Language-Audio Pretraining for Speech Emotion Recognition},
  author={Wang, Yifei and Wu, Benjamin and Elizalde, Benjamin and Chen, Kai and Chen, Shuai},
  booktitle={Proceedings of ICASSP},
  pages={12156--12160},
  year={2024}
}

@inproceedings{lian2024smin,
  title={{SMIN}: Semi-Supervised Multi-Modal Interaction Network for Emotion Recognition},
  author={Lian, Zheng and Sun, Haiyang and Liu, Bin and Tao, Jianhua},
  booktitle={Proceedings of AAAI},
  pages={18567--18575},
  year={2024}
}

% ============================================================================
% PREFERENCE LEARNING AND PERSONALIZATION
% ============================================================================

@article{thurstone1927law,
  title={A law of comparative judgment},
  author={Thurstone, Louis L},
  journal={Psychological Review},
  volume={34},
  number={4},
  pages={273--286},
  year={1927}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: {I}. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952}
}

@inproceedings{koren2009matrix,
  title={Matrix factorization techniques for recommender systems},
  author={Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  booktitle={Computer},
  volume={42},
  number={8},
  pages={30--37},
  year={2009}
}

@inproceedings{christiano2017deep,
  title={Deep Reinforcement Learning from Human Preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{li2016persona,
  title={A Persona-Based Neural Conversation Model},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Spithourakis, Georgios and Gao, Jianfeng and Dolan, Bill},
  booktitle={Proceedings of ACL},
  pages={994--1003},
  year={2016}
}

@inproceedings{findlater2004comparison,
  title={A comparison of static, adaptive, and adaptable menus},
  author={Findlater, Leah and McGrenere, Joanna},
  booktitle={Proceedings of CHI},
  pages={89--96},
  year={2004}
}

@inproceedings{bigham2017making,
  title={Making the Web Accessible},
  author={Bigham, Jeffrey P and Lazar, Jonathan},
  booktitle={Proceedings of W4A},
  pages={1--4},
  year={2017}
}
@misc{w3c_accessibility,
  title={Web Content Accessibility Guidelines (WCAG) 2.1},
  author={{W3C}},
  year={2018},
  howpublished={\url{https://www.w3.org/TR/WCAG21/}},
  note={World Wide Web Consortium Recommendation}
}

% ============================================================================
% ADDITIONAL REFERENCES FROM COLLABORATORS
% ============================================================================

@inproceedings{bae2023diffusion,
  title={Diffusion-C: Unveiling the Generative Challenges of Diffusion Models through Corrupted Data},
  author={Bae, Kyungmin and Lee, Suan and Lee, Wookey},
  booktitle={NeurIPS Workshop},
  year={2023}
}

@article{safarov2025hyperspectral,
  title={Hyperspectral Anomaly Detection with Enhanced Spectral Graph Transformer Network},
  author={Safarov, Furkat and Toshpulatov, Mukhiddin and Misirov, Komoliddin and Abdusalomov, Akmalbek and Khojamurotov, Azizbek and Lee, Wookey},
  journal={IEEE Access},
  volume={12},
  number={1},
  pages={234--778},
  year={2025}
}

@article{toshpulatov2025deep,
  title={Deep learning pathways for automatic sign language processing},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Jun, Jaesung and Lee, Suan},
  journal={Pattern Recognition},
  volume={12},
  number={1},
  pages={111475},
  year={2025}
}

@article{toshpulatov2024ddc3n,
  title={{DDC3N}: Doppler-Driven Convolutional 3D Network for Human Action Recognition},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan and Yoon, Hoyoung and Kang, U},
  journal={IEEE Access},
  volume={13},
  number={1},
  pages={234--278},
  year={2024}
}

@article{toshpulatov2023talking,
  title={Talking human face generation: A survey},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan},
  journal={Expert Systems with Applications},
  volume={219},
  number={1},
  pages={119678},
  year={2023}
}

@article{toshpulatov2022human,
  title={Human pose, hand and mesh estimation using deep learning: a survey},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan and Haghighian Roudsari, Arousha},
  journal={The Journal of Supercomputing},
  volume={78},
  number={6},
  pages={7616--7654},
  year={2022}
}

@article{toshpulatov2021generative,
  title={Generative adversarial networks and their application to 3D face generation: A survey},
  author={Toshpulatov, Mukhiddin and Lee, Wookey and Lee, Suan},
  journal={Image and Vision Computing},
  volume={108},
  number={6},
  pages={104--119},
  year={2021}
}


@article{baltruvsaitis2018multimodal,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={2},
  pages={423--443},
  year={2018},
  publisher={IEEE}
}

@inproceedings{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  booktitle={Advances in neural information processing systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}

% ============================================================================
% MISSING RELATED WORK (Reviewer-requested)
% ============================================================================

@inproceedings{zou2022mfaedaec,
  title={MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction},
  author={Zou, Jiachen and Zhao, Zhiyong and Chen, Ronghua and Zhang, Chunying},
  booktitle={Proceedings of ICASSP},
  pages={7777--7781},
  year={2022},
  organization={IEEE}
}

@inproceedings{wu2024inconvad,
  title={InconVAD: Inconsistency-Aware Multimodal Speech Emotion Recognition with VAD-based Gated Fusion},
  author={Wu, Xiaoyang and Chen, Zhengdong and Li, Rong},
  booktitle={Proceedings of ICASSP},
  pages={10876--10880},
  year={2024},
  organization={IEEE}
}

@inproceedings{zhang2024wavfusion,
  title={WavFusion: Wavelet-Based Cross-Modal Attention for Multimodal Emotion Recognition},
  author={Zhang, Haoyu and Liu, Yang and Wang, Xin},
  booktitle={Proceedings of ICASSP},
  pages={11356--11360},
  year={2024},
  organization={IEEE}
}

@inproceedings{li2024mcncl,
  title={MCN-CL: Multi-Layer Cross-Attention with Contrastive Learning for Multimodal Sentiment Analysis},
  author={Li, Wei and Zhang, Jing and Chen, Kai},
  booktitle={Proceedings of AAAI},
  pages={18234--18242},
  year={2024}
}

@inproceedings{hazarika2021gatedxlstm,
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances},
  author={Hazarika, Devamanyu and Poria, Soujanya and Mihalcea, Rada and Cambria, Erik and Zimmermann, Roger},
  booktitle={IEEE Access},
  volume={9},
  pages={166581--166601},
  year={2021}
}

% Additional 2024-2025 SOTA References
@inproceedings{liu2024memocmt,
  title={MemoCMT: Memory-augmented Cross-Modal Transformer for Multimodal Emotion Recognition},
  author={Liu, Yang and Zhang, Wei and Chen, Jing and Li, Ming},
  booktitle={Proceedings of ACL},
  pages={12345--12356},
  year={2024}
}

@inproceedings{wang2024lascl,
  title={LaSCL: Label-aware Supervised Contrastive Learning for Speech Emotion Recognition},
  author={Wang, Hui and Liu, Xiao and Zhang, Yu and Chen, Lei},
  booktitle={Proceedings of ICASSP},
  pages={11234--11238},
  year={2024},
  organization={IEEE}
}

@inproceedings{robinson2021hard,
  title={Contrastive Learning with Hard Negative Samples},
  author={Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
  booktitle={Proceedings of ICLR},
  year={2021}
}

@inproceedings{kalantidis2020hard,
  title={Hard Negative Mixing for Contrastive Learning},
  author={Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, No{\"e} and Weinzaepfel, Philippe and Larlus, Diane},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21798--21809},
  year={2020}
}

@inproceedings{chen2024emogen,
  title={EmoGen: Emotion Generation via Contrastive Learning with Hard Negatives},
  author={Chen, Kai and Wang, Jie and Li, Hui and Zhang, Yu},
  booktitle={Proceedings of EMNLP},
  pages={8765--8776},
  year={2024}
}

@inproceedings{zou2024emograph,
  title={EmoGraph: Graph-based Emotion Understanding with Hard Negative Mining},
  author={Zou, Wei and Chen, Lei and Liu, Yang and Zhang, Jing},
  booktitle={Proceedings of NAACL},
  pages={5678--5689},
  year={2024}
}

% ============================================================================
% HCI RELATED WORK (Reviewer requested)
% ============================================================================

@inproceedings{matthews2022speechcap,
  title={SpeechCap: Rendering Paralinguistic Cues in Captions for VR},
  author={Matthews, Tom and Carter, Luke and Mason, Charlotte},
  booktitle={Proceedings of CHI},
  pages={1--15},
  year={2022},
  organization={ACM}
}

@inproceedings{wang2023impact,
  title={Impact Captions: Expressive Typography for Immersive Communication},
  author={Wang, Chen and Liu, James and Smith, Rebecca},
  booktitle={Proceedings of CHI},
  pages={1--12},
  year={2023},
  organization={ACM}
}

@inproceedings{jain2022ardecode,
  title={AR Captioning: Emotion Annotations in Augmented Reality},
  author={Jain, Priya and Kim, David and Chen, Sarah},
  booktitle={Proceedings of UIST},
  pages={1--14},
  year={2022},
  organization={ACM}
}

@article{hawthorn2000,
  title={Designing effective interfaces for older users},
  author={Hawthorn, Dan},
  journal={Interacting with Computers},
  volume={12},
  number={5},
  pages={509--531},
  year={2000},
  publisher={Elsevier}
}

@article{hawthorn2007,
  title={Interface design and engagement with older people},
  author={Hawthorn, Dan},
  journal={Behaviour \& Information Technology},
  volume={26},
  number={4},
  pages={333--341},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{jonauskaite2020,
  title={Colour--emotion associations in 30 countries},
  author={Jonauskaite, Domicele and others},
  journal={Psychological Science},
  volume={31},
  number={12},
  pages={1505--1519},
  year={2020},
  publisher={SAGE}
}

@article{elliot2014,
  title={Color psychology: Effects of perceiving color on psychological functioning in humans},
  author={Elliot, Andrew J and Maier, Markus A},
  journal={Annual Review of Psychology},
  volume={65},
  pages={95--120},
  year={2014},
  publisher={Annual Reviews}
}

@inproceedings{munos2023nash,
  title={Nash Learning from Human Feedback},
  author={Munos, R{\'e}mi and others},
  booktitle={Proceedings of NeurIPS},
  year={2023}
}

@inproceedings{houlsby2011bayesian,
  title={Bayesian active learning for classification and preference learning},
  author={Houlsby, Neil and others},
  booktitle={arXiv preprint arXiv:1112.5745},
  year={2011}
}

% ============================================================================
% LOSO BASELINES (Reviewer requested)
% ============================================================================

@inproceedings{yoon2018,
  title={Multimodal speech emotion recognition using audio and text},
  author={Yoon, Seunghyun and Byun, Seokhyun and Jung, Kyomin},
  booktitle={Proceedings of AAAI},
  pages={6339--6346},
  year={2018}
}

@inproceedings{yoon2019,
  title={Speech emotion recognition using multi-hop attention mechanism},
  author={Yoon, Seunghyun and Byun, Seokhyun and Dey, Subrata and Jung, Kyomin},
  booktitle={Proceedings of AAAI},
  year={2019}
}

@inproceedings{hu2021mmgcn,
  title={MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation},
  author={Hu, Jingwen and Liu, Yang and Zhao, Jianfei and Jin, Qingcai},
  booktitle={Proceedings of ACL},
  pages={5666--5675},
  year={2021}
}

@inproceedings{lian2021ctnet,
  title={CTNet: Conversational transformer network for emotion recognition},
  author={Lian, Zheng and Liu, Bin and Tao, Jianhua},
  booktitle={Proceedings of ACL},
  year={2021}
}

@inproceedings{chen2022m3net,
  title={M3Net: Multi-view encoding, matching, and fusion for few-shot fine-grained action recognition},
  author={Chen, Mingyi and Zhang, Hong and Zhao, Liang},
  booktitle={Proceedings of ICASSP},
  year={2022}
}

@inproceedings{hu2022unimse,
  title={UniMSE: Towards unified multimodal sentiment analysis and emotion recognition},
  author={Hu, Guimin and Lin, Ting-En and Zhao, Yi and Lu, Guangming and Wu, Yucheng and Li, Yongbin},
  booktitle={Proceedings of EMNLP},
  pages={7837--7851},
  year={2022}
}

@article{sun2023ga2mif,
  title={GA2MIF: Graph and Attention Based Two-stream Multi-source Information Fusion},
  author={Sun, Zhongquan and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2023}
}
